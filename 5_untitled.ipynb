{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"df_predictions.csv\")\n",
    "#df = df.set_index(['player_name', 'position', 'team_title', 'event', 'opponent_team_title']).rename(columns={'position.1':'position'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df\n",
    "\n",
    "# Calculate absolute error\n",
    "df_predictions['absolute_error'] = abs(df_predictions['predicted_points'] - df_predictions['actual_points'])\n",
    "df_predictions = df_predictions.drop(columns={'predicted_points','actual_points'}).reset_index(drop=True)\n",
    "\n",
    "# Groupby at two levels to get: a) Predictions for all positions by gameweek (i.e. overall level)\n",
    "df_predictions_overall = df_predictions.drop(columns='position').groupby(['dataset','gameweek']).agg(['mean']).reset_index()\n",
    "df_predictions_overall.insert(2, 'position', 'overall')\n",
    "\n",
    "# b) Predictions by gameweek and by position\n",
    "df_predictions_by_position = df_predictions.groupby(['dataset','gameweek','position']).agg(['mean']).reset_index()\n",
    "\n",
    "# Concatenate to get full grouped predictions\n",
    "df_predictions_grouped = pd.concat([df_predictions_overall, df_predictions_by_position])\n",
    "df_predictions_grouped.columns = df_predictions_grouped.columns.droplevel(1)\n",
    "df_predictions_grouped = df_predictions_grouped.rename(columns={'absolute_error':'mean_absolute_error'})\n",
    "\n",
    "df_predictions_grouped.to_csv('df_predictions_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>gameweek</th>\n",
       "      <th>position</th>\n",
       "      <th>event</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>overall</td>\n",
       "      <td>5.129147</td>\n",
       "      <td>2.860811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>overall</td>\n",
       "      <td>6.085202</td>\n",
       "      <td>2.198158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>overall</td>\n",
       "      <td>7.041237</td>\n",
       "      <td>1.987693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>overall</td>\n",
       "      <td>8.012313</td>\n",
       "      <td>1.922248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "      <td>overall</td>\n",
       "      <td>8.958198</td>\n",
       "      <td>1.916588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>training</td>\n",
       "      <td>37</td>\n",
       "      <td>midfielder</td>\n",
       "      <td>19.763992</td>\n",
       "      <td>1.613621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>training</td>\n",
       "      <td>38</td>\n",
       "      <td>defender</td>\n",
       "      <td>20.219419</td>\n",
       "      <td>2.023379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>training</td>\n",
       "      <td>38</td>\n",
       "      <td>forward</td>\n",
       "      <td>20.806944</td>\n",
       "      <td>1.946664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>training</td>\n",
       "      <td>38</td>\n",
       "      <td>goalkeeper</td>\n",
       "      <td>20.689947</td>\n",
       "      <td>2.153495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>training</td>\n",
       "      <td>38</td>\n",
       "      <td>midfielder</td>\n",
       "      <td>20.257901</td>\n",
       "      <td>1.610959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset  gameweek    position      event  mean_absolute_error\n",
       "0        test         3     overall   5.129147             2.860811\n",
       "1        test         4     overall   6.085202             2.198158\n",
       "2        test         5     overall   7.041237             1.987693\n",
       "3        test         6     overall   8.012313             1.922248\n",
       "4        test         7     overall   8.958198             1.916588\n",
       "..        ...       ...         ...        ...                  ...\n",
       "283  training        37  midfielder  19.763992             1.613621\n",
       "284  training        38    defender  20.219419             2.023379\n",
       "285  training        38     forward  20.806944             1.946664\n",
       "286  training        38  goalkeeper  20.689947             2.153495\n",
       "287  training        38  midfielder  20.257901             1.610959\n",
       "\n",
       "[360 rows x 5 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-10, 1.41747416e-10, 2.00923300e-10, 2.84803587e-10,\n",
       "       4.03701726e-10, 5.72236766e-10, 8.11130831e-10, 1.14975700e-09,\n",
       "       1.62975083e-09, 2.31012970e-09, 3.27454916e-09, 4.64158883e-09,\n",
       "       6.57933225e-09, 9.32603347e-09, 1.32194115e-08, 1.87381742e-08,\n",
       "       2.65608778e-08, 3.76493581e-08, 5.33669923e-08, 7.56463328e-08,\n",
       "       1.07226722e-07, 1.51991108e-07, 2.15443469e-07, 3.05385551e-07,\n",
       "       4.32876128e-07, 6.13590727e-07, 8.69749003e-07, 1.23284674e-06,\n",
       "       1.74752840e-06, 2.47707636e-06, 3.51119173e-06, 4.97702356e-06,\n",
       "       7.05480231e-06, 1.00000000e-05, 1.41747416e-05, 2.00923300e-05,\n",
       "       2.84803587e-05, 4.03701726e-05, 5.72236766e-05, 8.11130831e-05,\n",
       "       1.14975700e-04, 1.62975083e-04, 2.31012970e-04, 3.27454916e-04,\n",
       "       4.64158883e-04, 6.57933225e-04, 9.32603347e-04, 1.32194115e-03,\n",
       "       1.87381742e-03, 2.65608778e-03, 3.76493581e-03, 5.33669923e-03,\n",
       "       7.56463328e-03, 1.07226722e-02, 1.51991108e-02, 2.15443469e-02,\n",
       "       3.05385551e-02, 4.32876128e-02, 6.13590727e-02, 8.69749003e-02,\n",
       "       1.23284674e-01, 1.74752840e-01, 2.47707636e-01, 3.51119173e-01,\n",
       "       4.97702356e-01, 7.05480231e-01, 1.00000000e+00, 1.41747416e+00,\n",
       "       2.00923300e+00, 2.84803587e+00, 4.03701726e+00, 5.72236766e+00,\n",
       "       8.11130831e+00, 1.14975700e+01, 1.62975083e+01, 2.31012970e+01,\n",
       "       3.27454916e+01, 4.64158883e+01, 6.57933225e+01, 9.32603347e+01,\n",
       "       1.32194115e+02, 1.87381742e+02, 2.65608778e+02, 3.76493581e+02,\n",
       "       5.33669923e+02, 7.56463328e+02, 1.07226722e+03, 1.51991108e+03,\n",
       "       2.15443469e+03, 3.05385551e+03, 4.32876128e+03, 6.13590727e+03,\n",
       "       8.69749003e+03, 1.23284674e+04, 1.74752840e+04, 2.47707636e+04,\n",
       "       3.51119173e+04, 4.97702356e+04, 7.05480231e+04, 1.00000000e+05])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.logspace(-10, 5, 100)\n",
    "\n",
    "# perform 10 cross fold validation for 100 different alpha values...\n",
    "# picks out best alpha value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old interpreting xgboost v2 doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting XGBoost V2\n",
    "\n",
    "Undoubtedly, XGBv2 was the best performing model out of the models tested so far, and so for that reason XGBv2 will be the model which we deploy. Before deploying XGBv2, however, I want to spend some time **understanding what features are generating the model's predictions**. \n",
    "\n",
    "Also, if possible, I'm hoping we'll be able to see what's causing the sharp error spikes mentioned above. However, if do not get a definitive answer to this question, it should be noted that I will proceed and still deploy the final model. The reason for this is because I soon go back to University. And at this moment, therefore, I believe it would be best to focus on deploying \"something\". I'm aware this is not best practice; however, I want to learn more about how we actually deploy ML algorithms in the real-world - as well as researching them, which is what we have done in this notebook. \n",
    "\n",
    "With all that said, let's define some new functions ```get_preds_xgboostv2()``` and ```simulate_xgboostv2()``` to interpret XGBv2*. \n",
    "\n",
    "*[*These functions are replicates of what we had earlier, however, they now collect the features/settings used to generate the predictions each gameweek.]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_xgboostv2(gameweek):\n",
    "\n",
    "    \"\"\"Returns the predictions made for a given simulated gameweek \n",
    "    for XGBv2.\n",
    "\n",
    "    :param: int64 gameweek: The first gameweek in the test range. \n",
    "            E.g. if we're at the start of gameweek 5, we make \n",
    "            predictions for gameweeks 5,6,7,8 and 9. \n",
    "\n",
    "    :rtype: DataFrame train: Training data predictions obtained via.\n",
    "            10 Fold Cross Validation.\n",
    "            DataFrame test: Training data predictions obtained via.\n",
    "            10 Fold Cross Validation.\n",
    "            DataFrame df_gw_param_used: Parameters used to generate the \n",
    "            predictions for the GW.\n",
    "            DataFrame df_gw_feat_importance: Importance of each of the\n",
    "            variables(/features) used for the GW. \n",
    "    \"\"\"    \n",
    "\n",
    "    # Initialise gameweek ranges\n",
    "    prev_gw = gameweek-1\n",
    "    all_gameweeks = list(range(0,prev_gw+6))\n",
    "    train_gameweeks = list(range(0,prev_gw+1))\n",
    "    test_gameweeks = list(range(prev_gw+1,prev_gw+6))\n",
    "\n",
    "    # Get all gameweeks in both sets of ranges\n",
    "    df_all_gameweeks = df[(df.index.get_level_values('event').isin(all_gameweeks))]\n",
    "\n",
    "    # Rename target variable\n",
    "    df_all_gameweeks = df_all_gameweeks.rename(columns={'total_points':'total_points_actual'})\n",
    "\n",
    "    # Standardise the independent variables\n",
    "    df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'] = StandardScaler().fit_transform(\n",
    "                                            df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'])\n",
    "\n",
    "    # Train-Test split the data\n",
    "    train = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(train_gameweeks))]\n",
    "    test = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(test_gameweeks))]\n",
    "\n",
    "    # Define independent and dependent variables\n",
    "    X_train = train.loc[:, train.columns != 'total_points_actual']\n",
    "    y_train_actual = train['total_points_actual']     \n",
    "    X_test = test.loc[:, train.columns != 'total_points_actual']\n",
    "    y_test_actual = test['total_points_actual'] \n",
    "\n",
    "    # Initialise XGBoost\n",
    "    xbgr = xgb.XGBRegressor()\n",
    "\n",
    "    # Setup search heuristic using parameter grid from earlier\n",
    "    sh = HalvingGridSearchCV(xbgr, param_grid, cv = 5, factor = 5, \n",
    "                    min_resources ='exhaust', n_jobs = -1, verbose = 2, random_state = 42).fit(X_train, y_train_actual)\n",
    "\n",
    "    # Fit XGBoost best estimator to training data \n",
    "    model = sh.best_estimator_.fit(X_train, y_train_actual) \n",
    "\n",
    "    # Define parameters used / and their importance\n",
    "    param_used = sh.best_params_\n",
    "    feat_importance = model.feature_importances_\n",
    "\n",
    "    # Define DataFrames to be outputted\n",
    "    df_gw_feat_importance = pd.DataFrame({'gameweek':gameweek, 'feature':X_train.columns, 'importance':feat_importance})\n",
    "    df_gw_param_used = pd.DataFrame({'gameweek':gameweek, 'value':param_used}).reset_index().rename(columns={'index':'parameter'})[[\n",
    "                                                                                            'gameweek', 'parameter', 'value']]\n",
    "    # Make predictions    \n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train_actual, cv=10)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Create prediction column for train/test DataFrames\n",
    "    train['total_points_predicted'] = y_train_pred\n",
    "    test['total_points_predicted'] = y_test_pred\n",
    "\n",
    "    return train, test, df_gw_param_used, df_gw_feat_importance\n",
    "\n",
    "\n",
    "def simulate_xgboostv2():\n",
    "    \n",
    "    \"\"\"Returns the predictions made for every gameweek and every subdataset\n",
    "    for XGBv2 - as well as: the parameters used each gameweek and the \n",
    "    importance of the features.\n",
    "\n",
    "    :rtype: DataFrame df_predictions_grouped: Stores the predictions made for \n",
    "            every gameweek and every subdataset/submodel.\n",
    "            DataFrame df_predictions_grouped: Stores the predictions made for \n",
    "            every gameweek and every subdataset/submodel.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    gw_predictions_array = []\n",
    "    gw_param_used_array = [] \n",
    "    gw_feat_importance_array = [] \n",
    "\n",
    "    # For every gameweek in the season\n",
    "    for gw in tqdm(list(range(3,39))):\n",
    "        \n",
    "        # Simulate predictions for the gameweek \n",
    "        train, test, df_gw_param_used, df_gw_feat_importance = get_preds_xgboostv2(gw) \n",
    "\n",
    "        # Manipulate training predictions\n",
    "        df_train_predictions = train.reset_index()\n",
    "        df_train_predictions['dataset'] = 'training'\n",
    "        df_train_predictions['gameweek'] = gw\n",
    "        df_train_predictions = df_train_predictions.rename(columns = {'total_points_predicted':'predicted_points',\n",
    "                                                                      'total_points_actual':'actual_points'})\n",
    "        df_train_predictions = df_train_predictions[['dataset','gameweek','position','predicted_points', 'actual_points']]\n",
    "        \n",
    "        # Manipulate test predictions\n",
    "        df_test_predictions = test.reset_index()\n",
    "        df_test_predictions['dataset'] = 'test'\n",
    "        df_test_predictions['gameweek'] = gw\n",
    "        df_test_predictions = df_test_predictions.rename(columns = {'total_points_predicted':'predicted_points',\n",
    "                                                                    'total_points_actual':'actual_points'})\n",
    "        df_test_predictions = df_test_predictions[['dataset','gameweek','position','predicted_points', 'actual_points']]\n",
    "\n",
    "        # Concatenate & append\n",
    "        df_gw_predictions = pd.concat([df_train_predictions, df_test_predictions])\n",
    "        gw_predictions_array.append(df_gw_predictions)\n",
    "        gw_param_used_array.append(df_gw_param_used)\n",
    "        gw_feat_importance_array.append(df_gw_feat_importance)\n",
    "\n",
    "    # Concatenate and append\n",
    "    df_predictions = pd.concat(gw_predictions_array)\n",
    "    df_param_used = pd.concat(gw_param_used_array)\n",
    "    df_feat_importance = pd.concat(gw_feat_importance_array)\n",
    "    \n",
    "    # Calculate absolute error\n",
    "    df_predictions['absolute_error'] = abs(df_predictions['predicted_points'] - df_predictions['actual_points'])\n",
    "    df_predictions = df_predictions.drop(columns={'predicted_points','actual_points'}).reset_index(drop=True)\n",
    "\n",
    "    # Groupby at two levels to get: a) Predictions for all positions by gameweek (i.e. overall level)\n",
    "    df_predictions_overall = df_predictions.drop(columns='position').groupby(['dataset','gameweek']).agg(['mean']).reset_index()\n",
    "    df_predictions_overall.insert(2, 'position', 'overall')\n",
    "\n",
    "    # b) Predictions by gameweek and by position\n",
    "    df_predictions_by_position = df_predictions.groupby(['dataset','gameweek','position']).agg(['mean']).reset_index()\n",
    "\n",
    "    # Concatenate to get full grouped predictions\n",
    "    df_predictions_grouped = pd.concat([df_predictions_overall, df_predictions_by_position])\n",
    "    df_predictions_grouped.columns = df_predictions_grouped.columns.droplevel(1)\n",
    "    df_predictions_grouped = df_predictions_grouped.rename(columns={'absolute_error':'xgboost_v2_mae'})\n",
    "    \n",
    "    return df_predictions_grouped, df_param_used, df_feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XGBoost V2 simulation again, however, this time we return the params used/feature importance for each GW\n",
    "df_predictions_grouped, df_param_used, df_feat_importance = simulate_xgboostv2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shap\n",
    "\n",
    "### GW11 and GW15 spikes\n",
    "\n",
    "To start with, I'd like to look at the first spikes which occured during GWs 11 and 15. And we will use **SHAP values** in particular as our feature attribution technique of choice. These seemed to be a good place to start based on my research (See [Interpretable Machine Learning with XGBoost](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) and [Explain Your Model with the SHAP Values](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d) for more).\n",
    "\n",
    "In particular, let's look at the SHAP values in GWs 9-17 (from the XGBv2 model). This will allow us to examine the feature importances before, during and after both spikes. Any changes throughout this period might point us toward what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise shap ranks array\n",
    "shap_importance_array = []\n",
    "\n",
    "# For every GW in the period we're investigating\n",
    "for gameweek in range(9,18):\n",
    "\n",
    "    # Initialise gameweek ranges\n",
    "    prev_gw = gameweek-1\n",
    "    all_gameweeks = list(range(0,prev_gw+6))\n",
    "    train_gameweeks = list(range(0,prev_gw+1))\n",
    "    test_gameweeks = list(range(prev_gw+1,prev_gw+6))\n",
    "\n",
    "    # Get all gameweeks in both sets of ranges\n",
    "    df_all_gameweeks = df[(df.index.get_level_values('event').isin(all_gameweeks))]\n",
    "\n",
    "    # Rename target variable\n",
    "    df_all_gameweeks = df_all_gameweeks.rename(columns={'total_points':'total_points_actual'})\n",
    "\n",
    "    # Standardise the independent variables\n",
    "    df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'] = StandardScaler().fit_transform(\n",
    "                                            df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'])\n",
    "\n",
    "    # Train-Test split the data\n",
    "    train = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(train_gameweeks))]\n",
    "    test = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(test_gameweeks))]\n",
    "\n",
    "    # Define independent and dependent variables\n",
    "    X_train = train.loc[:, train.columns != 'total_points_actual']\n",
    "    y_train_actual = train['total_points_actual']     \n",
    "    X_test = test.loc[:, train.columns != 'total_points_actual']\n",
    "    y_test_actual = test['total_points_actual'] \n",
    "\n",
    "    # Initialise XGBoost\n",
    "    xbgr = xgb.XGBRegressor()\n",
    "\n",
    "    # Setup search heuristic using parameter grid from earlier\n",
    "    sh = HalvingGridSearchCV(xbgr, param_grid, cv = 5, factor = 5, \n",
    "                    min_resources ='exhaust', n_jobs = -1, verbose = 2, random_state = 42).fit(X_train, y_train_actual);\n",
    "    \n",
    "    # Fit XGBoost best estimator to training data \n",
    "    model = sh.best_estimator_.fit(X_train, y_train_actual) \n",
    "\n",
    "    # Calculate SHAP values\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(X_test)\n",
    "    \n",
    "    # SHAP values DataFrame \n",
    "    df_shap_values = pd.DataFrame(shap_values, columns = X_test.columns)\n",
    "\n",
    "    # 'Sum of SHAP value magnitudes' over all samples to get feature importance \n",
    "    df_shap_importance = pd.DataFrame(abs(df_shap_values).sum().sort_values(ascending=False), \n",
    "                                                  columns=['importance_gw'+str(gameweek)]).reset_index()\n",
    "    # Append to array outside loop\n",
    "    shap_importance_array.append(df_shap_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SHAP value importances for all the GWs\n",
    "for i in range(0,9):\n",
    "    if i in [0,1]:\n",
    "        df_shap_importance_by_gw = pd.merge(shap_importance_array[0], shap_importance_array[1], on =['index'])\n",
    "    else:\n",
    "        df_shap_importance_by_gw = pd.merge(df_shap_importance_by_gw, shap_importance_array[i], on =['index'])\n",
    "\n",
    "# Calculate relative feature importances\n",
    "df_rel_importance = pd.DataFrame(df_shap_importance_by_gw['index'])\n",
    "for col in df_shap_importance_by_gw.columns[1:]:\n",
    "    df_rel_importance['rel_'+col] = df_shap_importance_by_gw[col]/df_shap_importance_by_gw[col].sum()*100\n",
    "\n",
    "# Split DataFrame into 'spike' and 'nonspike' GWs / Then take averages\n",
    "df_spike_gws = df_rel_importance[['index', 'rel_importance_gw11', 'rel_importance_gw15']]\n",
    "df_nonspike_gws = df_rel_importance[df_rel_importance.columns.drop(['rel_importance_gw11', 'rel_importance_gw15'])]\n",
    "df_nonspike_gws['mean_rel_importance_nonspike_gws'] = df_nonspike_gws.mean(axis=1)\n",
    "\n",
    "# Compare SHAP values across both sets of GWs\n",
    "df_compare = pd.merge(df_spike_gws, df_nonspike_gws[['index', 'mean_rel_importance_nonspike_gws']], on = 'index', how='left')\n",
    "df_compare.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate SHAP ranks\n",
    "df_shap_ranks = df_compare\n",
    "for col in df_shap_ranks.columns[1:]:\n",
    "    df_shap_ranks['rank_'+col] = df_shap_ranks[col].rank(ascending=False)\n",
    "df_shap_ranks.head(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
