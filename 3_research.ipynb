{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# Now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# SFS\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Sciki-learn models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# SHAP values\n",
    "import shap\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "# Timings\n",
    "%load_ext autotime\n",
    "# %unload_ext autotime\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will look at a range of different machine learning algorithms and begin to evaluate their respective performance in predicting fantasy football points. We will achieve this by simulation. That is, we will simulate and compare how different models would have performed on *last season’s data*. Our choice of evaluation metric will be the **Mean Absolute Error** (MAE) between the predictions made and the observed fantasy football points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "i. [Idea behind simulation](#IdeaBehindSimulation)\n",
    "\n",
    "<br>\n",
    "\n",
    "[Linear Models](#LinearModels)\n",
    "\n",
    "i. [Linear Regression](#LR)\n",
    "\n",
    "ii. [Ridge Regression and LASSO Regression](#RidgeAndLASSO)\n",
    "\n",
    "<br>\n",
    "\n",
    "[Non-linear Models](#NonLinearModels)\n",
    "\n",
    "i. [Random Forest](#RF)\n",
    "\n",
    "ii. [XGBoost](#XGB)\n",
    "\n",
    "iii. [XGBoost V2](#XGBV2)\n",
    "\n",
    "<br>\n",
    "\n",
    "[Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='IdeaBehindSimulation'></a>\n",
    "## Idea behind simulation\n",
    "\n",
    "For a given model, the idea will be to simulate the model as if it were deployed at the beginning of last season. In other words, for every gameweek in the season, the model will make predictions (as if we were \"in that gameweek\") for that gameweek and ensuing gameweeks. But, what will this look like in practice? Let's give an example:\n",
    "\n",
    "**Example: Simulating Gameweek *N***\n",
    "\n",
    "Assume we're in Gameweek *N* last season. That is, Gameweeks *1, 2, ... , N-1* have happened and Gameweek *N* is about to be played. Then, \n",
    "* The model will be **trained** on Gameweeks *1, 2, ... , N-1* (i.e. Gameweeks *1, 2, ... , N-1* will form the training data).\n",
    "* And the model will **predict** Gameweeks *N, N+1, N+2, N+3, N+4* (i.e. Gameweeks *N, N+1, N+2, N+3, N+4* will form the test data).\n",
    "\n",
    "for *N* between *3, 4, ... , 38*.\n",
    "\n",
    "<br>\n",
    "\n",
    "*A couple of things to note...* \n",
    "\n",
    "1) **Why 5?:** It should be noted, that the model always makes predictions for the next 5 gameweeks. But why 5? My thinking here is that, in practice, a model which predicts for just the next gameweek is not that useful. Conversely, a model which makes predictions for *lots and lots* gameweeks ahead of time might start to make inaccurate estimates. Therefore, for now, I think 5 serves as a 'happy medium'.\n",
    "\n",
    "2) **What about Gameweeks (GWs) 1 and 2?:** You may have noticed that we will not make predictions for GWs 1 and 2. I've chosen to exclude making predictions for these GWs for a few reasons:\n",
    "* By **GW1** (and by design) all predictor variables will be null (Recall: the predictor variables reflect data observed *before* the start of the given GW).\n",
    "* By **GW2** (and by the same design) we will still not have observed a \"full\" Gameweek. Yes, we will have seen points scored in GW1. However, the predictor variables for GW1 will still be null.\n",
    "* For now, I think **it is acceptable to not make predictions for these Gameweeks**. I think this is ok because our models will still \"work\" (that is - make predictions) for 36 out of the 38 Gameweeks in the season. However, of course, this is something which should be addressed in a future iteration of the project...\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in cleaned data from local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the cleaned dataset we've put together so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Redefine Index\n",
    "df = pd.read_csv(\"/Users/samharrison/Documents/data_sci/fpl_points_predictor/data/cleaned_data_2020_21_season.csv\")\n",
    "df = df.set_index(['player_name','position','team_title','event','opponent_team_title'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Note: (You may have already noticed) The feature **chance_of_playing_next_round** is not always reflective of the gameweek defined. In actual fact, the feature represents the probability at the time the data was gathered. This is down to how the FPL API works. Therefore, we will have to drop the feature from any simulations ran below since **it contains incorrect values.**]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For this section of the notebook, we're only interested in past observations. Therefore, we drop any gameweeks in \n",
    "# the future which haven't happened yet\n",
    "df = df.dropna(subset=['total_points'])\n",
    "\n",
    "# We drop the feature mentioned above as well as 'value' since we do not need it for this section\n",
    "df = df.drop(columns={'finished','value','chance_of_playing_next_round'})\n",
    "\n",
    "print(df.shape)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LinearModels'></a>\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, I'd like to see how far we can get just by using linear models. I'm more familiar with linear models having studied them as part of my degree (in particular, Linear Regression), so I think it would be cool to put some of this theory into action.\n",
    "\n",
    "For the linear models tried below, I believe stronger predictions will be made by first **partitioning the dataset by playing position** into smaller “subdatasets”. Namely, I've made the decision to create and train individual submodels for each playing position, in the hope of achieving better estimates. \n",
    "\n",
    "To illustrate my thinking, it should be noted that in fantasy football, players from different positions score points differently (e.g. goalkeepers and defenders score +4 points for a clean sheet, while midfielders score just +1 point, and forwards score 0 points). Therefore, I believe linear models, at least, will produce better estimates when each model/submodel is trained on just the one position specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LR'></a>\n",
    "## Linear Regression\n",
    "For the first model, we'll try Linear Regression (LR). LR should provide two useful insights:\n",
    "\n",
    "***Insight 1)*** *By examining the LR coeffecients, we can understand which variables are significant when predicting fantasy football points.*\n",
    "\n",
    "***Insight 2)***  *LR will provide ‘baseline’ predictions which future models can be benchmarked against.*\n",
    "\n",
    "<br>\n",
    "\n",
    "Before we running any model, however, we must first deal with the *multicollinearity* within our dataset. We will achieve this by taking advantage of **Sequential Feature Selction (SFS)**. SFS is a search algorithm which put simply looks for the features most relevant to the problem. \n",
    "\n",
    "Now, with multicollinearity and the above two insights in mind, let's define some methodology:\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "*Part 1: Understanding the coeffecients* \n",
    "\n",
    "The insight we'll be focusing on in this part will be Insight 1).\n",
    "1. On <u>all of last season's data</u>, run SFS to choose the **most relevant features.**\n",
    "2. Using the relevant features, run LR for each position and visualize the **significance of each coeffecient.**\n",
    "\n",
    "*Part 2: Making Predictions*\n",
    "\n",
    "The insight we'll be focusing on in this part will be Insight 2).\n",
    "\n",
    "1. Simulate model on last season's data\n",
    "    * For every Gameweek *3, 4, ... , 38*:\n",
    "        * Using SFS, choose the most relevant features for the GW.\n",
    "2. Analyse predictions\n",
    "\n",
    "*[Note: The goal of Part 1 here is just to gain a feel for which variables might be important. That's why we're using all of last season's data in this part. The alternative would be to look at coeffecient signifance on a gameweek-by-gameweek basis. For now, however, I want to keep things simple.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression // Part 1: Understanding the coeffecients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our subdatasets. And we'll wrap this step as a function for whenever we need to redefine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def define_player_subdatasets(df):\n",
    "\n",
    "    # Note, for the goalkeeper subdataset we also drop irrelevant attacking player metrics like goals_WMA etc.\n",
    "    df_goalkeeper = df[df['goalkeeper_flag']==1].drop(columns=['goalkeeper_flag','defender_flag','midfielder_flag','forward_flag', \n",
    "                                                           'goals_WMA','shots_WMA','xG_WMA','xA_WMA','assists_WMA','key_passes_WMA',\n",
    "                                                           'npg_WMA','npxG_WMA','goals_pgw','shots_pgw','xG_pgw','xA_pgw', \n",
    "                                                           'assists_pgw', 'key_passes_pgw', 'npg_pgw', 'npxG_pgw'])\n",
    "    df_defender = df[df['defender_flag']==1].drop(columns=['goalkeeper_flag','defender_flag','midfielder_flag','forward_flag'])\n",
    "    df_midfielder = df[df['midfielder_flag']==1].drop(columns=['goalkeeper_flag','defender_flag','midfielder_flag','forward_flag'])\n",
    "    df_forward = df[df['forward_flag']==1].drop(columns=['goalkeeper_flag','defender_flag','midfielder_flag','forward_flag'])\n",
    "\n",
    "    print('Goalkeeper shape:'+str(df_goalkeeper.shape)+'\\nDefender shape:'+str(df_defender.shape)+\n",
    "          '\\nMidfielder shape:'+str(df_midfielder.shape)+'\\nForward shape:'+str(df_forward.shape))\n",
    "    \n",
    "    return df_goalkeeper, df_defender, df_midfielder, df_forward\n",
    "\n",
    "df_goalkeeper, df_defender, df_midfielder, df_forward = define_player_subdatasets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_feat_selector(pos):\n",
    "    \n",
    "    \"\"\"Uses Sequential Feature Selection to select the most \n",
    "    relevant features. \n",
    "    \n",
    "    :param: str pos: Subdataset.\n",
    "    :rtype: array list(best_features.iloc[0]): Chosen features.\n",
    "    \"\"\"  \n",
    "\n",
    "    # Initialise subdataset dict\n",
    "    df_by_position_dict = {'goalkeeper':df_goalkeeper, 'defender':df_defender, \n",
    "                           'midfielder':df_midfielder, 'forward':df_forward}\n",
    "    # Initialise subdataset\n",
    "    df_position = df_by_position_dict[pos]\n",
    "\n",
    "    # Standardise independent variables\n",
    "    df_position.loc[:, df_position.columns != 'total_points'] = StandardScaler().fit_transform(\n",
    "                                                            df_position.loc[:, df_position.columns != 'total_points'])\n",
    "\n",
    "    # Define independent/dependent vars\n",
    "    X = df_position.loc[:, 'home_flag':'opponent_goals_against_pgw']\n",
    "    y = df_position['total_points']\n",
    "    \n",
    "    # Initialise LR model\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    # Run SFS\n",
    "    sfs = SFS(lr, \n",
    "              k_features = len(X.columns), \n",
    "              forward = True, \n",
    "              floating = False, \n",
    "              scoring = 'neg_mean_squared_error',\n",
    "              cv = 10)\n",
    "\n",
    "    # Fit the model\n",
    "    sfs = sfs.fit(X, y)\n",
    "\n",
    "    # DataFrame which has score(/best features) for each k\n",
    "    df_k_scores = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "    # Select best k and therefore best features\n",
    "    best_features = df_k_scores[df_k_scores['avg_score'] == df_k_scores['avg_score'].max()]['feature_names']\n",
    "\n",
    "    # Print out chosen features\n",
    "    print(\"Relevant features for \" + pos.capitalize() + \" dataset: \" )\n",
    "    for i in list(best_features.iloc[0]):\n",
    "        print(i)\n",
    "    print(\"\")\n",
    "\n",
    "    return list(best_features.iloc[0])\n",
    "\n",
    "goalkeeper_feat = seq_feat_selector('goalkeeper')\n",
    "defender_feat = seq_feat_selector('defender')\n",
    "midfielder_feat = seq_feat_selector('midfielder')\n",
    "forward_feat = seq_feat_selector('forward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefining our subdatasets and looking at their shapes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine subdatasets\n",
    "df_goalkeeper = df_goalkeeper[goalkeeper_feat+['total_points']]\n",
    "df_defender = df_defender[defender_feat+['total_points']]\n",
    "df_midfielder = df_midfielder[midfielder_feat+['total_points']]\n",
    "df_forward = df_forward[forward_feat+['total_points']]\n",
    "\n",
    "print('Goalkeeper shape:'+str(df_goalkeeper.shape)+'\\nDefender shape:'+str(df_defender.shape)+\n",
    "      '\\nMidfielder shape:'+str(df_midfielder.shape)+'\\nForward shape:'+str(df_forward.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LR on all of last season's data using the chosen features just found, let's visualize the coeffecients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_significant_features(pos):\n",
    "    \n",
    "    \"\"\"Visualizes the signifance of the chosen features. \n",
    "    \n",
    "    :param: str position: Subdataset.\n",
    "    :rtype: None\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Initialise\n",
    "    df_by_position_dict = {'goalkeeper':df_goalkeeper, 'defender':df_defender, \n",
    "                           'midfielder':df_midfielder, 'forward':df_forward}\n",
    "    df_position = df_by_position_dict[pos]\n",
    "\n",
    "    # Standardise independent variables\n",
    "    df_position.loc[:, df_position.columns != 'total_points'] = StandardScaler().fit_transform(\n",
    "                                                            df_position.loc[:, df_position.columns != 'total_points'])\n",
    "\n",
    "    # Define independent and dependent variables\n",
    "    X = df_position.loc[:, ~df_position.columns.isin(['total_points'])]\n",
    "    y = df_position['total_points']\n",
    "\n",
    "    # Fit model  \n",
    "    model = LinearRegression().fit(X, y)\n",
    "\n",
    "    # Make DataFrame w/ coeffecients\n",
    "    df_plot = pd.DataFrame({'values': model.coef_, 'variable': X.columns}).sort_values(by='values') #, columns=['values'])\n",
    "    df_plot = df_plot.set_index('variable')\n",
    "\n",
    "    # Formatting \n",
    "    plt.figure(figsize=(21,7))\n",
    "    plt.title(pos.capitalize()+\" dataset: Coeffecient Significance\"+\"\\nR-squared: \"+str(round(model.score(X,y),3)), fontsize=16)\n",
    "    plt.xlabel(\"Variable\", fontsize=14)\n",
    "    plt.ylabel(\"Coeffecient\", fontsize=14)\n",
    "\n",
    "    # Plot\n",
    "    df_plot['values'].plot(kind='bar',color=(df_plot['values'] > 0).map({True: 'g', False: 'r'}))\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    return \n",
    "\n",
    "viz_significant_features('goalkeeper')\n",
    "viz_significant_features('defender')\n",
    "viz_significant_features('midfielder')\n",
    "viz_significant_features('forward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude Part 1, let's answer Insight 1) and look at the findings:\n",
    "\n",
    "1. All submodels had a R-squared value of around ~17% apart from the defender submodel which has a considerably lower R-squared value of ~10%.\n",
    "2. Notably, **time_pgw** was a signifcant variable in most submodels.\n",
    "    * This shouldn't come as a surprise - players need to play to gain FPL points!\n",
    "   \n",
    "   \n",
    "3. Interestingly, **xGChain_pgw** was important for goalkeepers and **xGBuildup_WMA** was important for defenders. \n",
    "    * These variables capture how frequent players are involved in the buildup play to shots/goals. Hence it's an interesting observation that goalkeepers and defenders more involved in the buildup play are likely to gain more FPL points.\n",
    "\n",
    "\n",
    "4. Unexpectedly, **assists_pgw** and **xA_pgw** had negative coeffecients in the midfielder submodel.\n",
    "    * This definitely came as a surprise as you would expect the variables to have positive coeffecients. After doing some research however (See [Oh No! I Got the Wrong Sign! What should I do](http://www.stat.columbia.edu/~gelman/stuff_for_blog/oh_no_I_got_the_wrong_sign.pdf) for more), I've decided to continue instead of investigating further. There is a multitude of reasons which perhaps explain why these variables have the opposite sign than expected, and therefore, I think this is best left for future investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression // Part 2: Making predictions\n",
    "\n",
    "Now, let's make some predictions on last season's data. In particular, let's simulate how LR would have performed as if the model(s) were deployed at the beginning of last season. **Recall:** The methodology for the LR simulation is basically to use Sequential Feature Selection. SFS will be used to choose the relevant features for each position and each gameweek.\n",
    "\n",
    "Before running any simulations, however, we must redefine our subdatasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_goalkeeper, df_defender, df_midfielder, df_forward = define_player_subdatasets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate model on last season's data: Linear Regression\n",
    "Let's define some functions ```get_preds_linear_model()``` and ```simulate_linear_model()``` to simulate how our LR model(s) would have performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_linear_model(gameweek, position, model_str):\n",
    "\n",
    "    \"\"\"Returns the predictions made for a given simulated gameweek and \n",
    "    (position) subdataset. \n",
    "\n",
    "    :param: int64 gameweek: The first gameweek in the test range. E.g. if \n",
    "            we're at the start of gameweek 5, we make predictions for gameweeks \n",
    "            5,6,7,8 and 9. \n",
    "            str position: The subdataset/submodel we're running. \n",
    "            str model: What model are we using to generate predictions?\n",
    "            \n",
    "    :rtype: float64 y_train_pred: Training data predictions obtained via.\n",
    "            10 Fold Cross Validation.\n",
    "            int64 y_train: Training data target variable actual values.\n",
    "            float64 y_test_pred: Test data predictions obtained via.\n",
    "            model trained on the training data.\n",
    "            int64 y_test: Test data target variable actual values.\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    # Initialise gameweek ranges\n",
    "    prev_gw = gameweek-1\n",
    "    all_gameweeks = list(range(0,prev_gw+6))\n",
    "    train_gameweeks = list(range(0,prev_gw+1))\n",
    "    test_gameweeks = list(range(prev_gw+1,prev_gw+6))\n",
    "\n",
    "    # Initiaise position subdataset\n",
    "    df_by_position_dict = {'goalkeeper':df_goalkeeper, 'defender':df_defender, \n",
    "                           'midfielder':df_midfielder, 'forward':df_forward}\n",
    "    df_position = df_by_position_dict[position]\n",
    "\n",
    "    # Get all gameweeks in both sets of ranges\n",
    "    df_all_gameweeks = df_position[(df_position.index.get_level_values('event').isin(all_gameweeks))]\n",
    "\n",
    "    # Rename target variable\n",
    "    df_all_gameweeks = df_all_gameweeks.rename(columns={'total_points':'total_points_actual'})\n",
    "\n",
    "    # Standardise the independent/predictor variableS\n",
    "    df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'] = StandardScaler().fit_transform(\n",
    "                                            df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'])\n",
    "\n",
    "    # Train-Test split the data\n",
    "    train = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(train_gameweeks))]\n",
    "    test = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(test_gameweeks))]\n",
    "\n",
    "    # Standardise the dependent/target variable (i.e. we will inverse this later)\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaler.fit(train[['total_points_actual']])\n",
    "    y_train = target_scaler.transform(train[['total_points_actual']])\n",
    "    y_test = target_scaler.transform(test[['total_points_actual']])\n",
    "    \n",
    "    # Define independent/predictor variables\n",
    "    X_train = train.loc[:, train.columns != 'total_points_actual']\n",
    "    X_test = test.loc[:, test.columns != 'total_points_actual'] \n",
    "    \n",
    "    # Which model are we simulating?...\n",
    "    if model_str == 'Linear Regression': \n",
    "        # Initialise LR model\n",
    "        lr = LinearRegression()\n",
    "\n",
    "        # Run SFS\n",
    "        sfs = SFS(lr, \n",
    "                  k_features = len(X_train.columns), \n",
    "                  forward = True, \n",
    "                  floating = False, \n",
    "                  scoring = 'neg_mean_squared_error',\n",
    "                  cv = 10)\n",
    "\n",
    "        # Fit the model\n",
    "        sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "        # DataFrame which has score(/best features) for each k\n",
    "        df_k_scores = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "        # Select best k and therefore best features\n",
    "        best_features = df_k_scores[df_k_scores['avg_score'] == df_k_scores['avg_score'].max()]['feature_names']\n",
    "\n",
    "        # Redefine test/training dataset with best features\n",
    "        X_train = X_train[list(best_features.iloc[0])]\n",
    "        X_test = X_test[list(best_features.iloc[0])]\n",
    "\n",
    "        # Note: This part is just a quick check to remove any highly correlated feature pairs . In other words, SFS\n",
    "        # doesn't always guarantee that highly correlated feature pairs are removed - which I learned the hard way!\n",
    "        bivar_corr = pd.DataFrame(X_train.corr().stack()).reset_index()\n",
    "        feat_to_excl =  bivar_corr[(bivar_corr[0]>0.999)&(bivar_corr['level_0']!=bivar_corr['level_1'])]['level_0'].to_list()\n",
    "        X_train = X_train[[feat for feat in X_train.columns if feat not in feat_to_excl]]\n",
    "        X_test = X_test[[feat for feat in X_test.columns if feat not in feat_to_excl]]\n",
    "\n",
    "        # Initialise Linear Regression\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "      \n",
    "    elif model_str == 'Ridge Regression':\n",
    "        # Create Ridge regression with multiple alphas - then fit to training data\n",
    "        regr_cv = RidgeCV(alphas = np.logspace(-10, 5, 100), cv=10)\n",
    "        ridge_cv = regr_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Find the best alpha using Cross Validation\n",
    "        best_alpha = ridge_cv.alpha_\n",
    "        model = RidgeCV(alphas = [best_alpha]).fit(X_train, y_train)\n",
    "        \n",
    "    elif model_str == 'LASSO Regression':\n",
    "        # Create LASSO regression with multiple alphas - then fit to training data\n",
    "        regr_cv = LassoCV(alphas = np.logspace(-10, 5, 100), cv=10)\n",
    "        lasso_cv = regr_cv.fit(X_train, y_train)\n",
    "\n",
    "        # Find the best alpha using Cross Validation\n",
    "        best_alpha = lasso_cv.alpha_\n",
    "        model = LassoCV(alphas = [best_alpha]).fit(X_train, y_train) \n",
    "    else:\n",
    "        pass\n",
    "    # Obtain predictions on training data via. Cross Validation \n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=10)\n",
    "            \n",
    "    # Invert transform on predictions for both datasets\n",
    "    y_test_pred = model.predict(X_test)    \n",
    "    \n",
    "    # Have to reshape when using LASSO (nuance)\n",
    "    if model_str == 'LASSO Regression':\n",
    "        y_test_pred = y_test_pred.reshape(-1,1)\n",
    "        y_train_pred = np.reshape(y_train_pred, (len(y_train_pred), 1))\n",
    "        \n",
    "    y_test_pred = target_scaler.inverse_transform(y_test_pred)\n",
    "    y_train_pred = target_scaler.inverse_transform(y_train_pred)\n",
    "    \n",
    "#     # Create prediction column for train/test DataFrames\n",
    "#     df_train['total_points_predicted'] = y_train_pred[0]\n",
    "#     df_test['total_points_predicted'] = y_test_pred[0]\n",
    "    \n",
    "    return y_train_pred, y_train, y_test_pred, y_test, X_train.index, X_test.index\n",
    "\n",
    "\n",
    "\n",
    "def simulate_linear_model(model_str):\n",
    "    \n",
    "    \"\"\"Returns the predictions made for every gameweek and every subdataset/\n",
    "    submodel, i.e. the full (simulated) Premier League 2020/2021 season.  \n",
    "\n",
    "    :param: model: What model are we using to generate predictions?\n",
    "    :rtype: DataFrame df_predictions_grouped: Stores the predictions made for \n",
    "            every gameweek and every subdataset/submodel.\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    pos_predictions_array = []\n",
    "    gw_predictions_array = []\n",
    "    predictions_array = []\n",
    "\n",
    "    # For every gameweek in the season\n",
    "    for gw in tqdm(list(range(3,39))):\n",
    "        \n",
    "        # For every position\n",
    "        for pos in ['goalkeeper', 'defender', 'midfielder', 'forward']:\n",
    "\n",
    "            # Simulate predictions for the gameweek \n",
    "            y_train_pred, y_train, y_test_pred, y_test, train_index, test_index = get_preds_linear_model(gw, pos, model_str) \n",
    "            \n",
    "            # Convert predictions to DataFrame\n",
    "            df_train_pos_predictions = pd.DataFrame({'dataset':'training', 'gameweek':gw, 'position':pos, \n",
    "                                                     'predicted_points':y_train_pred.flatten(), 'actual_points':y_train.flatten()},\n",
    "                                                     index= train_index)\n",
    "            df_test_pos_predictions = pd.DataFrame({'dataset':'test', 'gameweek':gw, 'position':pos, \n",
    "                                                    'predicted_points':y_test_pred.flatten(), 'actual_points':y_test.flatten()},\n",
    "                                                    index= test_index)\n",
    "            \n",
    "            # Concatenate & append\n",
    "            df_pos_predictions = pd.concat([df_train_pos_predictions, df_test_pos_predictions])\n",
    "            pos_predictions_array.append(df_pos_predictions)\n",
    "\n",
    "        # Concatenate and append\n",
    "        df_gw_predictions = pd.concat(pos_predictions_array)\n",
    "        predictions_array.append(df_gw_predictions)\n",
    "            \n",
    "    # Concatenate ALL predictions for the season into single DataFrame\n",
    "    df_predictions = pd.concat(predictions_array)\n",
    "        \n",
    "    # Concatenate ALL predictions for the season into single DataFrame\n",
    "    df_predictions = pd.concat(predictions_array)\n",
    "        \n",
    "    # Calculate absolute error\n",
    "    df_predictions['absolute_error'] = abs(df_predictions['predicted_points'] - df_predictions['actual_points'])\n",
    "    df_predictions = df_predictions.drop(columns={'predicted_points','actual_points'}).reset_index(drop=True)\n",
    "\n",
    "    # Groupby at two levels to get: a) Predictions for all positions by gameweek (i.e. overall level)\n",
    "    df_predictions_overall = df_predictions.drop(columns='position').groupby(['dataset','gameweek']).agg(['mean']).reset_index()\n",
    "    df_predictions_overall.insert(2, 'position', 'overall')\n",
    "\n",
    "    # b) Predictions by gameweek and by position\n",
    "    df_predictions_by_position = df_predictions.groupby(['dataset','gameweek','position']).agg(['mean']).reset_index()\n",
    "\n",
    "    # Concatenate to get full grouped predictions\n",
    "    df_predictions_grouped = pd.concat([df_predictions_overall, df_predictions_by_position])\n",
    "    df_predictions_grouped.columns = df_predictions_grouped.columns.droplevel(1)\n",
    "    df_predictions_grouped = df_predictions_grouped.rename(columns={'absolute_error':model_str.lower().replace(\" \",\"_\")+\"_mae\"})\n",
    "    \n",
    "    return df_predictions_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the simulation using Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise dictionary which will store predictions\n",
    "predictions_dict = {}\n",
    "\n",
    "# Run simulation\n",
    "df_predictions_grouped = simulate_linear_model(model_str = 'Linear Regression')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['Linear Regression'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions: Linear Regression\n",
    "\n",
    "To help analyse the predictions/simulations - I've created a straightforward [tool](https://public.tableau.com/app/profile/samuel.harrison2532/viz/model_simulation_analysis/Dashboard) within Tableau. I had some experience using the software while on placement - so I thought it'd be cool to continue using/learning the software as part of this project. \n",
    "\n",
    "Anyhow, let's summarise how Linear Regression performed:\n",
    " \n",
    "**Season-long Average Mean Absolute Errors**:\n",
    "\n",
    "| Training/Test | Position<br>subdataset | Linear Regression<br>Season-long<br>Avg. MAE (2 dp) |\n",
    "| --- | --- | --- |\n",
    "| Test | Overall | **1.86** |\n",
    "| Test | Goalkeepers | 2.12 |\n",
    "| Test | Defenders | 1.99 |\n",
    "| Test | Midfielders | 1.64 |\n",
    "| Test | Forward | 2.12 |\n",
    "\n",
    "* Using Linear Regression, we have found 'baseline' predictions as planned (recall ***Insight 2)***). Namely, we ended up with an Overall (i.e. all positions) Season-long average MAE of **1.86 points.**\n",
    "* On average, Midfielders were predicted the best while Goalkeepers/Forwards were predicted the worse. \n",
    "\n",
    "**Other insights**:\n",
    "\n",
    "* However, there is cause for concern for Forwards and Midfielders. Between GWs 25-28, **MAE increased sharply** in both positions. Namely, MAE increased by about 20% for Forwards and by about 9% for Mifielders over the course of the three gameweeks. We expect models to improve with more data hence this was unexpected.\n",
    "    \n",
    "\n",
    "\n",
    "<!-- ; error spiked between GWs 23-28, namely, MAE increased from 1.61 to 2.07 over the course of the five gameweeks. \n",
    " -->\n",
    "<!-- Analysis Outline:\n",
    "* High Level/Season-long analysis insights by overall/BestToWorst position\n",
    "* Overfitting? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RidgeAndLASSO'></a>\n",
    "## Ridge Regression and LASSO Regression\n",
    "Next, let's try some types of regularized regression: Ridge Regression (Ridge) and LASSO Regression (LASSO). \n",
    "\n",
    "The benefit of using these two types of regression is that they handle multicollinearity better than Linear Regression. Because of this, the setup for this next section will be much simpler. Hence, let's define the methodology:\n",
    "\n",
    "**Methodology** \n",
    "\n",
    "1. Parameter tuning: **How will we tune the parameter alpha $\\alpha$?**\n",
    "2. Simulate model on last season's data\n",
    "3. Analyse predictions\n",
    "\n",
    "<br>\n",
    "\n",
    "Before continuing, let's must redefine our subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_goalkeeper, df_defender, df_midfielder, df_forward = define_player_subdatasets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is alpha $\\alpha$?**\n",
    "\n",
    "As we have just said, there isn't much setup for Ridge and LASSO when compared to LR. Ridge and LASSO are, however, **parameterised by the tuning parameter denoted by alpha $\\alpha$**. This value adds a shrinkage penalty which - to put it briefly - penalizes large coefficients and makes sure coeffecients do not get too large. Statistically what this parameter does, therefore, is **add bias** to the predictions made in exchange for **reduced variance**. \n",
    "\n",
    "*[Note: I could talk more about both regularized regression models and the mathematical/statistical differences between the two; however, for now I'd like to keep this part brief. It's Summer 2021 at the time of completing this project and I'll soon be returning to University. The syllabus next semester covers these kind of statisical modelling topics (regression, bias, variance and estimators) in greater detail than what I've already learned. Hence, I'd like to wait until then before studying this material in greater depth!]*\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. How do we optimize $\\alpha$?**\n",
    "\n",
    "\n",
    "We'll optimize alpha $\\alpha$ **for each position and for each gameweek** by 10-Fold Cross Validation. Namely, we'll try n=100 different values spaced evenly on a log scale between 10^-10 and 10^5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate model on last season's data: Ridge Regression and LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the simulation using Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "df_predictions_grouped  = simulate_linear_model(model_str = 'Ridge Regression')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['Ridge Regression'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the simulation using LASSO Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "df_predictions_grouped = simulate_linear_model(model_str = 'LASSO Regression')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['LASSO Regression'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions: Ridge Regression and LASSO Regression\n",
    "\n",
    "Again making use of the Tableau [dashboard](https://public.tableau.com/app/profile/samuel.harrison2532/viz/model_simulation_analysis/Dashboard), let's summarise the performance of both Ridge Regression and LASSO Regression:\n",
    " \n",
    "**Season-long Average Mean Absolute Errors**:\n",
    "\n",
    "| Training/Test | Position<br>subdataset | Linear Regression<br>Season-long<br>Avg. MAE (2 dp) |Ridge Regression<br>Season-long<br>Avg. MAE (2 dp) |LASSO Regression<br>Season-long<br>Avg. MAE (2 dp) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Test | Overall     | **1.86** | **1.88**|**1.87** |\n",
    "| Test | Goalkeepers | 2.12     | 2.28    | 2.18    |\n",
    "| Test | Defenders   | 1.99     | 1.99    | 1.99    |\n",
    "| Test | Midfielders | 1.64     | 1.64    | 1.63    |\n",
    "| Test | Forward     | 2.12     | 2.18    | 2.14    |\n",
    "  \n",
    "* The regularized regression models performed similarly to Linear Regression. \n",
    "* However, **Linear Regression still performed the best** (in Overall Season-long Avg. MAE).\n",
    "* Similar to before, both regularized models predicted Midfielders the best.\n",
    "\n",
    "<!-- \n",
    "* Using Linear Regression, we have found 'baseline' predictions as planned - recall ***Insight 2)***. Namely, we ended up with an overall (i.e. for all positions) season-long average MAE of **1.88 points.**\n",
    "* On average, Midfielders were predicted the best while Goalkeepers were predicted the worse.  -->\n",
    "\n",
    "**Other insights**:\n",
    "\n",
    "* Unfortunately, for both Ridge and LASSO, MAE increased sharply between GWs 25-28 for Forwards and Midfielders - as we saw before with the Linear Regression model. Hence there is clearly something is going on here. I've done some investigating and came up with a potential explanation as to what might be happening:\n",
    "\n",
    "**Potential explanation for MAE spike between GWs 25-28 (for both Forwards and Midfielders):**\n",
    "\n",
    "* I think the root cause for this problem may well be **squad rotation**. Namely, I think there was lots of unexpected rotation amongst both Forwards and Midfielders, which may have impacted the models' predictions. Here's a couple reasons why I think teams may have rotated their Forwards/Midfielders around this period:\n",
    "    1. **Double Gameweeks:** GWs 24 and 25 were \"double gameweeks\", i.e. some teams had two games within one gameweek. And it's common knowledge that fixture congestion can lead to squad rotation.\n",
    "    2. **Champions League/Europa League:** Around GW25 began the knockout stages in the Champions League and the Europa League which again likely led to lots of unexpected squad rotation. In fact, 7 Premier League teams played in these knockout games (Man City, Man United, Liverpool, Chelsea, Arsenal, Spurs and Leicester) which you might expect to have exacerbated the impact.\n",
    "    3. **Why Forwards and Midfielders?:** Notice predictive accuracy for Goalkeepers and Defenders *did not worsen* as we saw for Forwards and Midfielders around the same period. I would argue this supports my hypothesis since we know Forwards/Midfielders tend to be rotated more than Goalkeepers/Defenders. \n",
    "\n",
    "*[Note: More rigorous future investigation should indeed be taken. For now, however, I plan on monitoring the models' performances next season and seeing if the same thing happens again.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NonLinearModels'></a>\n",
    "## Non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to some non-linear models. \n",
    "\n",
    "Since we're now using non-linear models, **we will no longer partition the dataset by playing position**. I'm hopeful the non-linear models tried below will capture the non-linear relationships between the predictor variables; in particular, the relationship between the position variables (e.g. defender_flag) and the rest of the predictor variables.\n",
    "\n",
    "<!-- To begin with, I'd like to see how far we can get just by using linear models. I'm more familiar with linear models having studied them as part of my degree (in particular, Linear Regression), so I think it would be cool to put some of this theory into action.\n",
    "\n",
    "For the linear models tried below, I believe stronger predictions will be made by first **partitioning the dataset by playing position** into smaller “subdatasets”. Namely, I've made the decision to create and train individual submodels for each playing position, in the hope of achieving better estimates. \n",
    "\n",
    "To illustrate my thinking, it should be noted that in fantasy football, players from different positions score points differently (e.g. goalkeepers and defenders score +4 points for a clean sheet, while midfielders score just +1 point, and forwards score 0 points). Therefore, I believe linear models, at least, will produce better estimates when each model/submodel is trained on just the one position specifically. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RF'></a>\n",
    "## Random Forest \n",
    "For the first non-linear model, we'll try a Random Forest (RF). Let's define the methodology:\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "1. On *all of last season's data*, use **RandomizedSearchCV** to narrow down the search space for the best possible parameters.\n",
    "2. Using the best parameters found from step 1, concentrate our search for the best parameters using **GridSearchCV**.\n",
    "\n",
    "Then, using the best settings (just found):\n",
    "\n",
    "3. Simulate model on last season's data\n",
    "4. Analyse predictions\n",
    "\n",
    "*[Note: This methodology was inspired by the article [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) written by Will Koehrsen. Random Forest has lots of settings hence I wasn't sure where exactly to begin. So thanks to Will for writing such a helpful, well-written article on the subject!]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized search\n",
    "First, let's create a parameter grid to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in Random Forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = df.loc[:, ~df.columns.isin(['total_points'])]\n",
    "y = df['total_points']\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation (4320 settings in total)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 30, cv = 3, verbose=2, \n",
    "                               random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the n=30 random samples, what were the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "Using the best parameters from the above search, let's concentrate our search for the best RF parameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [8, 10],\n",
    "    'max_features': [5, 7],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [5, 7],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rf_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the n=48 combinations in the grid, what were the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save down our estimate for the best possible RF settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best estimator for future reference\n",
    "rf_best_estimator = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate model on last season's data: Random Forest\n",
    "As mentioned earlier, we'll no longer be partioning the dataset by playing position since we're now testing non-linear models. With that said, let's define two new functions ```get_preds_non_linear_model()``` and ```simulate_non_linear_model()``` to help run the simulations for the following non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_non_linear_model(gameweek, model_str):\n",
    "\n",
    "    \"\"\"Returns the predictions made for a given simulated gameweek.\n",
    "    \n",
    "    :param: int64 gameweek: The first gameweek in the test range. E.g. if \n",
    "            we're at the start of gameweek 5, we make predictions for gameweeks \n",
    "            5,6,7,8 and 9. \n",
    "            str model: What model are we using to generate predictions?\n",
    "            \n",
    "    :rtype: DataFrame train: Training data predictions obtained via.\n",
    "            10 Fold Cross Validation.\n",
    "            DataFrame test: Training data predictions obtained via.\n",
    "            10 Fold Cross Validation.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Initialise gameweek ranges\n",
    "    prev_gw = gameweek-1\n",
    "    all_gameweeks = list(range(0,prev_gw+6))\n",
    "    train_gameweeks = list(range(0,prev_gw+1))\n",
    "    test_gameweeks = list(range(prev_gw+1,prev_gw+6))\n",
    "\n",
    "    # Get all gameweeks in both sets of ranges\n",
    "    df_all_gameweeks = df[(df.index.get_level_values('event').isin(all_gameweeks))]\n",
    "\n",
    "    # Rename target variable\n",
    "    df_all_gameweeks = df_all_gameweeks.rename(columns={'total_points':'total_points_actual'})\n",
    "\n",
    "    # Standardise the independent variables\n",
    "    df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'] = StandardScaler().fit_transform(\n",
    "                                            df_all_gameweeks.loc[:, df_all_gameweeks.columns != 'total_points_actual'])\n",
    "\n",
    "    # Train-Test split the data\n",
    "    train = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(train_gameweeks))]\n",
    "    test = df_all_gameweeks[(df_all_gameweeks.index.get_level_values('event').isin(test_gameweeks))]\n",
    "\n",
    "    # Define independent and dependent variables\n",
    "    X_train = train.loc[:, train.columns != 'total_points_actual']\n",
    "    y_train_actual = train['total_points_actual']     \n",
    "    X_test = test.loc[:, train.columns != 'total_points_actual']\n",
    "    y_test_actual = test['total_points_actual'] \n",
    "\n",
    "    # Which model are we simulating?...\n",
    "    if model_str == 'Random Forest':\n",
    "        # Create Random Forest with best settings \n",
    "        rf = rf_best_estimator\n",
    "\n",
    "        # Fit Random Forest to training data\n",
    "        model = rf.fit(X_train, y_train_actual)    \n",
    "        \n",
    "    elif model_str == 'XGBoost':\n",
    "        # Create XGBoost with best settings \n",
    "        xbgr = xgbr_best_estimator\n",
    "\n",
    "        # Fit XGBoost to training data\n",
    "        model = xbgr.fit(X_train, y_train_actual) \n",
    "        \n",
    "    elif model_str == 'XGBoost V2':\n",
    "        # Initialise XGBoost\n",
    "        xbgr = xgb.XGBRegressor()\n",
    "        \n",
    "        # Setup search heuristic using parameter grid from earlier\n",
    "        sh = HalvingGridSearchCV(xbgr, param_grid, cv = 5, factor = 5, \n",
    "                    min_resources ='exhaust', n_jobs = -1, verbose = 2, random_state = 42).fit(X_train, y_train_actual) \n",
    "        \n",
    "        # Fit XGBoost best estimator to training data\n",
    "        model = sh.best_estimator_.fit(X_train, y_train_actual) \n",
    "\n",
    "    # Make predictions    \n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train_actual, cv=10)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Create prediction column for train/test DataFrames\n",
    "    train['total_points_predicted'] = y_train_pred\n",
    "    test['total_points_predicted'] = y_test_pred\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def simulate_non_linear_model(model_str):\n",
    "    \n",
    "    \"\"\"Returns the predictions made for every gameweek and every subdataset/\n",
    "    submodel, i.e. the full (simulated) Premier League 2020/2021 season.  \n",
    "\n",
    "    :param: model: What model are we using to generate predictions?\n",
    "    :rtype: DataFrame df_predictions_grouped: Stores the predictions made for \n",
    "            every gameweek and every subdataset/submodel.\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    gw_predictions_array = []\n",
    "\n",
    "    # For every gameweek in the season\n",
    "    for gw in tqdm(list(range(3,39))):\n",
    "        \n",
    "        # Simulate predictions for the gameweek \n",
    "        train, test = get_preds_non_linear_model(gw, model_str) \n",
    "\n",
    "        # Manipulate training predictions\n",
    "        df_train_predictions = train.reset_index()\n",
    "        df_train_predictions['dataset'] = 'training'\n",
    "        df_train_predictions['gameweek'] = gw\n",
    "        df_train_predictions = df_train_predictions.rename(columns = {'total_points_predicted':'predicted_points',\n",
    "                                                                      'total_points_actual':'actual_points'})\n",
    "        df_train_predictions = df_train_predictions[['dataset','gameweek','position','predicted_points', 'actual_points']]\n",
    "        \n",
    "        # Manipulate test predictions\n",
    "        df_test_predictions = test.reset_index()\n",
    "        df_test_predictions['dataset'] = 'test'\n",
    "        df_test_predictions['gameweek'] = gw\n",
    "        df_test_predictions = df_test_predictions.rename(columns = {'total_points_predicted':'predicted_points',\n",
    "                                                                    'total_points_actual':'actual_points'})\n",
    "        df_test_predictions = df_test_predictions[['dataset','gameweek','position','predicted_points', 'actual_points']]\n",
    "\n",
    "        # Concatenate & append\n",
    "        df_gw_predictions = pd.concat([df_train_predictions, df_test_predictions])\n",
    "        gw_predictions_array.append(df_gw_predictions)\n",
    "\n",
    "    # Concatenate and append\n",
    "    df_predictions = pd.concat(gw_predictions_array)\n",
    "        \n",
    "    # Calculate absolute error\n",
    "    df_predictions['absolute_error'] = abs(df_predictions['predicted_points'] - df_predictions['actual_points'])\n",
    "    df_predictions = df_predictions.drop(columns={'predicted_points','actual_points'}).reset_index(drop=True)\n",
    "\n",
    "    # Groupby at two levels to get: a) Predictions for all positions by gameweek (i.e. overall level)\n",
    "    df_predictions_overall = df_predictions.drop(columns='position').groupby(['dataset','gameweek']).agg(['mean']).reset_index()\n",
    "    df_predictions_overall.insert(2, 'position', 'overall')\n",
    "\n",
    "    # b) Predictions by gameweek and by position\n",
    "    df_predictions_by_position = df_predictions.groupby(['dataset','gameweek','position']).agg(['mean']).reset_index()\n",
    "\n",
    "    # Concatenate to get full grouped predictions\n",
    "    df_predictions_grouped = pd.concat([df_predictions_overall, df_predictions_by_position])\n",
    "    df_predictions_grouped.columns = df_predictions_grouped.columns.droplevel(1)\n",
    "    df_predictions_grouped = df_predictions_grouped.rename(columns={'absolute_error':model_str.lower().replace(\" \",\"_\")+\"_mae\"})\n",
    "    \n",
    "    return df_predictions_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence let's run the simulation using Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "df_predictions_grouped = simulate_non_linear_model(model_str = 'Random Forest')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['Random Forest'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Season-long Average Mean Absolute Errors**:\n",
    "\n",
    "| Training/Test | Position<br>subdataset | Linear Regression<br>Season-long<br>Avg. MAE (2 dp) |Ridge Regression<br>Season-long<br>Avg. MAE (2 dp) |LASSO Regression<br>Season-long<br>Avg. MAE (2 dp) |Random Forest<br>Season-long<br>Avg. MAE (2 dp) |\n",
    "| --- | --- | --- | --- | --- |--- |\n",
    "| Test | Overall     | **1.86** | **1.88**|**1.87** |**1.83** |\n",
    "| Test | Goalkeepers | 2.12     | 2.28    | 2.18    | 2.11    |\n",
    "| Test | Defenders   | 1.99     | 1.99    | 1.99    | 1.98    |\n",
    "| Test | Midfielders | 1.64     | 1.64    | 1.63    | 1.62    |\n",
    "| Test | Forward     | 2.12     | 2.18    | 2.14    | 1.95    |\n",
    "  \n",
    "* **RF bettered all previous linear models'** Season-long Avg. MAE. across all positions.\n",
    "* Predictions for Forwards improved considerably using RF. Goalkeepers, however, still yield the worst estimates in general.\n",
    "\n",
    "<!-- \n",
    "* Using Linear Regression, we have found 'baseline' predictions as planned - recall ***Insight 2)***. Namely, we ended up with an overall (i.e. for all positions) season-long average MAE of **1.88 points.**\n",
    "* On average, Midfielders were predicted the best while Goalkeepers were predicted the worse.  -->\n",
    "\n",
    "**Other insights**:\n",
    "\n",
    "* Interestingly, **RF made better predictions during the earlier gameweeks** across all postions excluding Goalkeepers. \n",
    "    * When viewing across all positions, (i.e. Overall) Random Forest made considerably better predictions than all previous models across GWs 3-10. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='XGB'></a>\n",
    "## XGBoost\n",
    "Since RF was sucessful, I'd like to try another non-linear model. And after doing some reading, I've decided to try XGBoost (XGB) next. \n",
    "\n",
    "I'm hopeful we'll obtain the best predictions so far using XGB, as my reading suggests the algorithm is very powerful and one of the most popular within the community for large datasets which is what we have. Methodology-wise, I'd like to replicate what we did for RF. I'm aware there is a variety of cool hyperparameter optimization techniques out there (many of which look powerful), however, for now I'd like to start simple. \n",
    "\n",
    "**Methodology**\n",
    "\n",
    "1. On *all* of last season's data, use **RandomizedSearchCV** to narrow down the search space for the best possible parameters.\n",
    "2. Using the best parameters found from RandomizedSearchCV, concentrate our search for the best parameters using **GridSearchCV**.\n",
    "\n",
    "Then, using the best settings (just found):\n",
    "\n",
    "3. Simulate model on last season's data\n",
    "4. Analyse predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized search\n",
    "First, let's create a parameter grid to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "eta = np.linspace(0.01, 0.4, num = 11)\n",
    "\n",
    "# Minimum loss reduction required to make a further partition on a leaf node\n",
    "gamma = np.logspace(-10, 5, 100)\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [3, 4, 5, 6, 8, 10, 12, 15]\n",
    "\n",
    "# Minimum weight needed in a child \n",
    "min_child_weight = [1, 3, 5, 7, 10, 12, 15]\n",
    "\n",
    "# Subsample ratio of columns when constructing each tree\n",
    "colsample_bytree = [0.3, 0.4, 0.5 , 0.7] \n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'eta': eta,\n",
    "    'gamma': gamma, \n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight, \n",
    "    'colsample_bytree': colsample_bytree}\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = df.loc[:, ~df.columns.isin(['total_points'])]\n",
    "y = df['total_points']\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "xgbr = xgb.XGBRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation (7040 settings in total)\n",
    "xgbr_random = RandomizedSearchCV(estimator = xgbr, param_distributions = random_grid, n_iter = 500, cv = 3, verbose=2, \n",
    "                                 random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "xgbr_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the n=500 random samples, what were the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "Using the best parameters from the above search, let's concentrate our search for the best XGB parameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'eta': np.linspace(0.01, 0.1, num = 5),\n",
    "    'gamma': np.logspace(-10, -8, 5), \n",
    "    'max_depth': [3, 4, 6],\n",
    "    'min_child_weight': [10, 12, 15], \n",
    "    'colsample_bytree': [0.45, 0.5, 0.55]}\n",
    "\n",
    "# Create a based model\n",
    "xgbr = xgb.XGBRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "xgbr_grid = GridSearchCV(estimator = xgbr, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "xgbr_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the n=675 combinations in the grid, what were the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save down our estimate for the best possible XGB settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best estimator for future reference\n",
    "xgbr_best_estimator = xgbr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate model on last season's data: XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "df_predictions_grouped = simulate_non_linear_model(model_str = 'XGBoost')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['XGBoost'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Season-long Average Mean Absolute Errors**:\n",
    "\n",
    "| Training/Test | Position<br>subdataset | Linear Regression<br>Season-long<br>Avg. MAE (2 dp) |Ridge Regression<br>Season-long<br>Avg. MAE (2 dp) |LASSO Regression<br>Season-long<br>Avg. MAE (2 dp) |Random Forest<br>Season-long<br>Avg. MAE (2 dp) |XGBoost<br>Season-long<br>Avg. MAE (2 dp) |\n",
    "| --- | --- | --- | --- | --- |--- |--- |\n",
    "| Test | Overall     | **1.86** | **1.88**|**1.87** |**1.83** |**1.82** |\n",
    "| Test | Goalkeepers | 2.12     | 2.28    | 2.18    | 2.11    | 2.12    |\n",
    "| Test | Defenders   | 1.99     | 1.99    | 1.99    | 1.98    | 1.97    |\n",
    "| Test | Midfielders | 1.64     | 1.64    | 1.63    | 1.62    | 1.62    |\n",
    "| Test | Forward     | 2.12     | 2.18    | 2.14    | 1.95    | 1.94    |\n",
    "  \n",
    "* **XGB is the best-performing model so far** but only performed *slightly* better than RF.  \n",
    "\n",
    "<!-- \n",
    "* Using Linear Regression, we have found 'baseline' predictions as planned - recall ***Insight 2)***. Namely, we ended up with an overall (i.e. for all positions) season-long average MAE of **1.88 points.**\n",
    "* On average, Midfielders were predicted the best while Goalkeepers were predicted the worse.  -->\n",
    "\n",
    "**Other insights**:\n",
    "\n",
    "* However, when viewing Overall (i.e. all positions), XGB's MAE was better than RF's for 31 out of the 36 Gameweeks. In other words, **XGB was consistently better than RF**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='XGBV2'></a>\n",
    "## XGBoost V2\n",
    "\n",
    "XGB has performed the best out of all the models tested so far. Therefore, I'd like to try running the XGB algorithm again, but this time with a slightly different methodology. In particular, I'd like to see how we can do if we were to **retune the model's hyperparameters EVERY gameweek.** We'll denote this new model by XGBv2.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "1. Simulate model on last season's data\n",
    "    * For every Gameweek *3, 4, ... , 38*:\n",
    "        * Using the parameter grid found earlier, search for the best parameters using **HalvingGridSearchCV**.\n",
    "2. Analyse predictions\n",
    "\n",
    "*A couple of things to note...* \n",
    "\n",
    "1) **What are we actually doing here?:** In essence, all we're doing here is retuning the XGBoost model every Gameweek. Before, we found some \"good\" XGB settings which we then used for the whole season. However this time,  I'd like to see whether there's any benefit in retuning the model every Gameweek as this could easily be replicated in practice. \n",
    "\n",
    "2) **What is HalvingGridSearchCV and why are we using it?:** * drumroll * HalvingGridSearchCV is <u>really similar</u> to GridSearchCV! The search strategy does this by an iterative procedure known as \"halving\". Basically, what happens here is different settings(/canditates) are tested using a small amount of data. The best candidates are then tested using more data, and this procedure is iterated until a best candidate is found. The benefit of using HalvingGridSearchCV is that is **much faster**  than GridSearchCV, which is crucial since this makes the methodology practical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate model on last season's data: XGBoost  V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "df_predictions_grouped = simulate_non_linear_model(model_str = 'XGBoost V2')\n",
    "\n",
    "# Add to dictionary of predictions\n",
    "predictions_dict['XGBoost V2'] = df_predictions_grouped\n",
    "df_predictions_grouped.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions: XGBoost V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Season-long Average Mean Absolute Errors**:\n",
    "\n",
    "| Training/Test | Position<br>subdataset | Linear Regression<br>Season-long<br>Avg. MAE (2 dp) |Ridge Regression<br>Season-long<br>Avg. MAE (2 dp) |LASSO Regression<br>Season-long<br>Avg. MAE (2 dp) |Random Forest<br>Season-long<br>Avg. MAE (2 dp) |XGBoost<br>Season-long<br>Avg. MAE (2 dp) |XGBoost V2<br>Season-long<br>Avg. MAE (2 dp) |\n",
    "| --- | --- | --- | --- | --- |--- |--- |--- |\n",
    "| Test | Overall     | **1.86** | **1.88**|**1.87** |**1.83** |**1.82** |**1.74** |\n",
    "| Test | Goalkeepers | 2.12     | 2.28    | 2.18    | 2.11    | 2.12    | 2.09    |\n",
    "| Test | Defenders   | 1.99     | 1.99    | 1.99    | 1.98    | 1.97    | 1.89    |\n",
    "| Test | Midfielders | 1.64     | 1.64    | 1.63    | 1.62    | 1.62    | 1.55    |\n",
    "| Test | Forward     | 2.12     | 2.18    | 2.14    | 1.95    | 1.94    | 1.80    |\n",
    "  \n",
    "* Changing the methodology for XGBoost ***considerably* improved results** across most positions.\n",
    "    * In particular, XGBv2 bettered our baseline LR model by ~6.6%.\n",
    "\n",
    "**Other insights**:\n",
    "\n",
    "* When viewing across all positions, XGBv2 **experienced some sharp error spikes** throughout the season (GWs: 11, 15, 19, 20, 26, 27 and 29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer: Error spike investigation\n",
    "\n",
    "Before concluding this notebook/research, it should be noted that I have spent quite some time investigating the unusual error spikes observed in the predictions made by XGBv2. Some of the material which I looked at (and in chronological order) was:\n",
    "\n",
    "* The actual **parameters used** every GW and the **feature importances** (where feature importances were \"gain\" by default).\n",
    "* Then, after doing some more reading, I learned that consistency and accuracy are desirable properties for measures of feature importance (See [Interpretable Machine Learning with XGBoost](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27)). Hence, I tried using **SHAP values** instead as a method for feature attribution.\n",
    "\n",
    "Unfortunately, however, it became rather difficult when I tried to put this theory into practice. In fact, the project came to a sort of standstill while I was mulling over different methodologies which could work. \n",
    "\n",
    "Therefore, I have decided to **proceed with the project** and not investigate further. The reason being is that I soon return to University which I mentioned earlier on in the notebook. I'm aware that this is not 'best practice' as problems like this are akin to those frequently seen in industry; however, for my own personal development, I want to learn more about how we actually deploy ML algorithms in the real-world as well as researching them which is what we have done up until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "Let's recap what we've just done. For clarity, I'll bullet the areas which I feel were particularly important: \n",
    "\n",
    "* To begin with, we set up an **architecture for modelling and simulation** for both linear and non-linear models. This architecure was inspired by an [idea](#IdeaBehindSimulation) which we covered at the beginning. \n",
    "* We then came up with **methodologies to optimise model performance**, e.g., parameter tuning, sometimes taking advantage of things like search algorithms which could be used from packages online.\n",
    "* Next, we developed a **tool in Tableau to help evaluate the different models**. Using this tool we were able to:\n",
    "    * Gain a high-level overview to quickly summarise a given model's accuracy.\n",
    "    * Identify unexpected observations which could then be investigated. \n",
    "* Finally, we highlighted **several areas which could be researched/investigated more in the future** hence leaving room for future iterations of the project. \n",
    "\n",
    "Furthermore, we found that **XGBoost V2** (XGBv2) was the best performing model out of all of the ML algorithms tested, and that generally non-linear models were better than linear ones which suggests non-linear models are probably more well-equipped for this project. For this reason, unsurprisingly, XGBv2 will be the model we deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export simulations (predictions) to Tableau\n",
    "Here, we're just exporting the predictions made above into [Tableau](https://public.tableau.com/app/profile/samuel.harrison2532/viz/model_simulation_analysis/Dashboard) for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all predictions made by models\n",
    "df_model_simulation_output = pd.merge(predictions_dict['Linear Regression'], predictions_dict['Ridge Regression'],\n",
    "                                      on = ['dataset', 'gameweek', 'position'],\n",
    "                                      how = 'left')\n",
    "df_model_simulation_output = pd.merge(df_model_simulation_output, predictions_dict['LASSO Regression'],\n",
    "                                      on = ['dataset', 'gameweek', 'position'],\n",
    "                                      how = 'left')\n",
    "df_model_simulation_output = pd.merge(df_model_simulation_output, predictions_dict['Random Forest'],\n",
    "                                      on = ['dataset', 'gameweek', 'position'],\n",
    "                                      how = 'left')\n",
    "# Concatenate and export\n",
    "df_model_simulation_output.to_csv(\"data/df_model_simulation_output_2020_21_season.csv\")\n",
    "df_model_simulation_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
